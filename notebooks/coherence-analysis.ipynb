{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Analysis\n",
    "\n",
    "## Approach 1: Using Semantic Similarity Graph\n",
    "Implementation of Semantic Similarity Graph based on Putra et al. Class use to compute the coherence of a given text\n",
    "\n",
    "\n",
    "## Approach 2: Using Neural Network \n",
    "Utilizing LSTM and fine-tuned BERT architecture to generate a coherence score for song lyrics. The architecture will be train on real world lyrics and then compared it to generated lyrics by other ML tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from gensim) (1.23.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from gensim) (1.9.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ktrin\\miniconda3\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\n",
      "ERROR: No matching distribution found for transformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-2.8.7-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 32.5 MB/s eta 0:00:00\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-2.8.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install transformer\n",
    "!pip install networkx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import re\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# import nltk and gensim library for token level embeddings\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Preprocessing Tools\n",
    "Before we can start analzing the coherence of our text, we need to set up a way to tokenize the input. We will be using [NLTK punkt module](https://www.nltk.org/api/nltk.tokenize.punkt.html) to tokenize the sentences in our dataset. In Putra et al. the research group used a pretrained GloVe word vector to project a sentence into a vector representation. It is noted that a sentence consists of multiple words {w1, w2, w3, ..., wn } where each word is mapped to a vector space. We'll do the same to keep it consistent with the original intention. A future research in the field involves utilizing other embedding models such as word2vec, Elmo, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ktrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in punkt module\n",
    "# instantiate the tokenizer\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "gloVe_embedder = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "gloVe_embed_vectors_vocab = gloVe_embedder.index_to_key\n",
    "len(gloVe_embed_vectors_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloVe_embed_vectors_vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing preprocessing tools\n",
    "def text_to_tokens(text):\n",
    "  \"\"\"\n",
    "    Process input text into individual token per sentence\n",
    "  \"\"\"\n",
    "  processed_lyric_tokenzied = [i.lower().replace(',', '').split() for i in text.split(\"\\n\")]\n",
    "  \n",
    "  return processed_lyric_tokenzied\n",
    "\n",
    "def tokens_to_vectors(sentences, embedder=gloVe_embedder):\n",
    "  \"\"\"\n",
    "    Process each token into it embedded vector using pre-trained embedder\n",
    "  \"\"\"\n",
    "  return [[embedder[word] for word in s] for s in sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticGraph():\n",
    "  \"\"\"\n",
    "    Semantic Similar Graph Implementation\n",
    "  \"\"\"\n",
    "  def __init__(self, data=None):\n",
    "    self.lemma = []\n",
    "    self.vectors = []\n",
    "    self.G = None\n",
    "\n",
    "    if data != None:\n",
    "      self.set_document(data)\n",
    "\n",
    "  def build_graph(self):\n",
    "    self.G = nx.DiGraph()\n",
    "    self.set_nodes()\n",
    "    self.set_edges()\n",
    "\n",
    "\n",
    "  def set_document(self, text):\n",
    "    \"\"\"\n",
    "      Tokenize and embedd the tokens\n",
    "      Load the document into the system\n",
    "      Generate a node for each token index\n",
    "    \"\"\"\n",
    "    document = {\n",
    "        'lemma': [],\n",
    "        'vectors': []\n",
    "    }\n",
    "    document['lemma'] = text_to_tokens(text, True)\n",
    "    embeddings = tokens_to_vectors(text_to_tokens(text, False))\n",
    "    vectors = [np.mean(embedding, axis=0) for embedding in embeddings]\n",
    "    document['vectors'] = vectors\n",
    "    self.lemma = document['lemma']\n",
    "    self.vectors = document['vectors']\n",
    "    self.build_graph()\n",
    "\n",
    "\n",
    "  def set_nodes(self):\n",
    "    \"\"\"\n",
    "      Generate a node per sentence in the stored dictionary\n",
    "    \"\"\"\n",
    "    for idx, sentence in enumerate(self.lemma):\n",
    "      self.G.add_node(idx)\n",
    "\n",
    "\n",
    "  def set_edges(self):\n",
    "    \"\"\"\n",
    "      Class Abstract Function\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "  def print(self):\n",
    "    \"\"\"\n",
    "      Print the graph in the console\n",
    "    \"\"\"\n",
    "    labels = nx.get_edge_attributes(self.G, 'weight')\n",
    "    for key in labels:\n",
    "      labels[key] = round(labels[key], 3)\n",
    "    pos = nx.spring_layout(self.G)\n",
    "    nx.draw(self.G, pos, with_labels=True, font_weight='bold')\n",
    "    nx.draw_networkx_edge_labels(self.G, pos, edge_labels=labels)\n",
    "\n",
    "\n",
    "  def evaluate_coherence(self):\n",
    "    \"\"\"\n",
    "      Compute coherence score the entire document\n",
    "      Based on Putra et al.\n",
    "        - Average over number of sentences, and number of outgoing edges weights\n",
    "    \"\"\"\n",
    "    labels = nx.get_edge_attributes(self.G, 'weight')\n",
    "    if len(labels.keys()) == 0:\n",
    "      return 0\n",
    "    return np.mean(np.array(list(labels.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "class PAV(SemanticGraph):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Inherit the Semantic Graph Class\n",
    "        super().__init__(self)\n",
    "        self.alpha = 0\n",
    "\n",
    "\n",
    "    def set_alpha(self, _alpha):\n",
    "        self.alpha = _alpha\n",
    "        self.build_graph()\n",
    "\n",
    "\n",
    "    def set_edges(self):\n",
    "        \"\"\"\n",
    "            Polymorp the set_edges method\n",
    "            Define coherence as previous sentences give context to current sentence\n",
    "            For PAV:\n",
    "                1. compute the cosine similarity of setence pairs\n",
    "                2. Compute Unique Overlapping Terms (UOT)\n",
    "\n",
    "        \"\"\"\n",
    "        i = len(self.vectors) -1\n",
    "        while i > 0:\n",
    "            # cosine similarity\n",
    "            cosine = 1 - spatial.distance.cosine(self.vectors[i], self.vectors[i - 1])\n",
    "            \n",
    "            # for two sentences, determine the UOT\n",
    "            terms1 = set(self.lemma[i])\n",
    "            terms2 = set(self.lemma[i - 1])\n",
    "            unique = terms1.intersection(terms2)\n",
    "            common = terms1.union(terms2)\n",
    "            uot = 1.0 * len(unique) / len(common)\n",
    "\n",
    "            # compute the weight\n",
    "            weight = self.alpha * uot + (1 - self.alpha) * cosine\n",
    "            self.G.add_weighted_edges_from([(i, i - 1, weight)])\n",
    "            i -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSV(SemanticGraph):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "\n",
    "    def set_edges(self):\n",
    "        \"\"\"\n",
    "            Polymorp the set_edges method\n",
    "            Define coherence as the dependency among sentences\n",
    "            For SSV:\n",
    "                1. compute the weight as cosine similarity\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "\n",
    "        # for each sentence, \n",
    "        # loop through each other sentence to compute the weight\n",
    "        while i < len(self.lemma):\n",
    "            j = 0\n",
    "\n",
    "            current_related_idx = -1\n",
    "            current_related_weight = 0\n",
    "            while j < len(self.lemma):\n",
    "\n",
    "                # if the same sentence, simply ignore\n",
    "                if i == j:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                # compute the weight of both sentence\n",
    "                # the weights are penalized by the distance between two senteces (closer sentences are more prefer)\n",
    "                # keep only the most similar sentence among all sentences in a given text\n",
    "                weight = (1 - spatial.distance.cosing(self.vectors[i], self.vectors[j]) / abs(i-j))\n",
    "                if weight > current_related_weight:\n",
    "                    current_related_weight = weight\n",
    "                    current_related_idx = j\n",
    "\n",
    "            # update weights between two sentence nodes\n",
    "            # go to next sentence\n",
    "            self.G.add_weighted_edges_from([(i, current_related_idx, current_related_weight)])\n",
    "\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSV(SemanticGraph):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        self.theta = 0\n",
    "\n",
    "\n",
    "    def set_theta(self, _theta):\n",
    "        self.theta = _theta\n",
    "        self.build_graph()\n",
    "\n",
    "\n",
    "    def set_edges(self):\n",
    "        \"\"\"\n",
    "            Polymorp the set_edges method\n",
    "            Define coherence as the dependency among sentences\n",
    "            Similar to SSV but allow for multiple outgoing edges\n",
    "            For MSV:\n",
    "                1. compute the weight as cosine similarity\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        while i < len(self.lemma):\n",
    "            j = 0\n",
    "            while j < len(self.lemma):\n",
    "                if i == j:\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "                # compute cosine similarity weight between current sentence and all other sentences in the document\n",
    "                # the weights are penalized by the distance between two senteces (closer sentences are more prefer)\n",
    "                # if a the weight is high and pass a certain threshold, that edge is created\n",
    "                weight = (1 - spatial.distance.cosine(self.vectors[i], self.vectors[j])) / (abs(i - j))\n",
    "                if weight > self.theta:\n",
    "                    self.G.add_weighted_edges_from([(i, j, weight)])\n",
    "                j += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9845b488047c880b3243e38cff204b64842e109bc52f8ea3e8abc45a8d12589d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
